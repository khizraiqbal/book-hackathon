"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[758],{3872(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var t=i(4848),o=i(8453);const r={title:"Chapter 2 - Voice-to-Action \u2013 Speech Input, Command Parsing, ROS 2 Triggers",sidebar_position:2},s="Chapter 2: Voice-to-Action \u2013 Speech Input, Command Parsing, ROS 2 Triggers",a={id:"module4/chapter2_voice_to_action_speech_input_command_parsing_ros2_triggers",title:"Chapter 2 - Voice-to-Action \u2013 Speech Input, Command Parsing, ROS 2 Triggers",description:"Introduction",source:"@site/docs/module4/chapter2_voice_to_action_speech_input_command_parsing_ros2_triggers.md",sourceDirName:"module4",slug:"/module4/chapter2_voice_to_action_speech_input_command_parsing_ros2_triggers",permalink:"/docs/module4/chapter2_voice_to_action_speech_input_command_parsing_ros2_triggers",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4/chapter2_voice_to_action_speech_input_command_parsing_ros2_triggers.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Chapter 2 - Voice-to-Action \u2013 Speech Input, Command Parsing, ROS 2 Triggers",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Chapter 1 - Introduction to Vision-Language-Action (VLA) Systems",permalink:"/docs/module4/chapter1_introduction_to_vla_systems"},next:{title:"Chapter 3 - LLM Cognitive Planning \u2013 Natural Language \u2192 Action Sequences in ROS 2",permalink:"/docs/module4/chapter3_llm_cognitive_planning_natural_language_action_sequences_ros2"}},c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Speech Recognition Fundamentals",id:"speech-recognition-fundamentals",level:2},{value:"Audio Processing Pipeline",id:"audio-processing-pipeline",level:3},{value:"Speech Recognition APIs",id:"speech-recognition-apis",level:3},{value:"Command Parsing and Natural Language Processing",id:"command-parsing-and-natural-language-processing",level:2},{value:"Intent Recognition",id:"intent-recognition",level:3},{value:"Command Structure",id:"command-structure",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Action Server Communication",id:"action-server-communication",level:3},{value:"Implementation Considerations",id:"implementation-considerations",level:2},{value:"Error Handling and Feedback",id:"error-handling-and-feedback",level:3},{value:"Robustness and Reliability",id:"robustness-and-reliability",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-2-voice-to-action--speech-input-command-parsing-ros-2-triggers",children:"Chapter 2: Voice-to-Action \u2013 Speech Input, Command Parsing, ROS 2 Triggers"}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-Action represents a critical component in the Vision-Language-Action (VLA) paradigm, where natural language commands are transformed into executable actions within robotic systems. This chapter explores the integration of speech recognition with ROS 2 (Robot Operating System 2) to enable intuitive human-robot interaction through voice commands."}),"\n",(0,t.jsx)(n.p,{children:"In physical AI and humanoid robotics, the ability to interpret and execute voice commands creates a natural interface between humans and autonomous systems. By leveraging modern speech recognition APIs and mapping them to ROS 2 action servers, we can create responsive and accessible robotic platforms that understand and execute complex instructions."}),"\n",(0,t.jsx)(n.h2,{id:"speech-recognition-fundamentals",children:"Speech Recognition Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"audio-processing-pipeline",children:"Audio Processing Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"The voice-to-action pipeline begins with audio capture and processing. The system must:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Capture audio input from microphones or other audio sources"}),"\n",(0,t.jsx)(n.li,{children:"Apply noise reduction and filtering techniques"}),"\n",(0,t.jsx)(n.li,{children:"Convert analog audio signals to digital format"}),"\n",(0,t.jsx)(n.li,{children:"Perform feature extraction for speech recognition"}),"\n",(0,t.jsx)(n.li,{children:"Generate text transcripts of spoken commands"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"speech-recognition-apis",children:"Speech Recognition APIs"}),"\n",(0,t.jsx)(n.p,{children:"Modern speech recognition services like OpenAI's Whisper, Google's Speech-to-Text, or Microsoft's Speech Services provide high-accuracy transcription capabilities. These services can be integrated into ROS 2 nodes to convert spoken commands into text that can be processed by the robotic system."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example ROS 2 node for speech recognition\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport speech_recognition as sr\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n        self.publisher = self.create_publisher(String, 'voice_commands', 10)\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Start listening for voice commands\n        self.start_voice_recognition()\n\n    def start_voice_recognition(self):\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        self.recognizer.listen_in_background(self.microphone, self.voice_callback)\n\n    def voice_callback(self, recognizer, audio):\n        try:\n            text = recognizer.recognize_google(audio)\n            msg = String()\n            msg.data = text\n            self.publisher.publish(msg)\n        except sr.UnknownValueError:\n            self.get_logger().info('Could not understand audio')\n        except sr.RequestError as e:\n            self.get_logger().error(f'Error: {e}')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"command-parsing-and-natural-language-processing",children:"Command Parsing and Natural Language Processing"}),"\n",(0,t.jsx)(n.h3,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Once speech is converted to text, the system must identify the user's intent. This involves:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Entity Extraction"}),": Identifying objects, locations, and parameters mentioned in the command"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Classification"}),": Determining the type of action requested"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parameter Resolution"}),": Converting natural language references to specific values"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"command-structure",children:"Command Structure"}),"\n",(0,t.jsx)(n.p,{children:"Voice commands typically follow patterns that can be parsed and understood by the system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'[Action] [Object] [Location/Parameters]\nExample: "Move the red block to the table"\n'})}),"\n",(0,t.jsx)(n.p,{children:"A command parser can use regular expressions, natural language processing libraries, or machine learning models to extract structured information from natural language commands."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import Dict, Optional\n\nclass CommandParser:\n    def __init__(self):\n        self.action_patterns = {\n            'move': r'move|go to|navigate to|travel to',\n            'pick': r'pick up|grab|take|lift',\n            'place': r'place|put|set|drop',\n            'grasp': r'grasp|hold|catch',\n            'release': r'release|let go|drop'\n        }\n\n    def parse_command(self, text: str) -> Optional[Dict]:\n        text_lower = text.lower()\n\n        # Extract action\n        action = None\n        for cmd_type, pattern in self.action_patterns.items():\n            if re.search(pattern, text_lower):\n                action = cmd_type\n                break\n\n        if not action:\n            return None\n\n        # Extract object and location\n        # This is a simplified example - real implementations would use more sophisticated NLP\n        remaining_text = re.sub(r'|'.join(self.action_patterns.values()), '', text_lower)\n\n        return {\n            'action': action,\n            'raw_command': text,\n            'parameters': remaining_text.strip()\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,t.jsx)(n.h3,{id:"action-server-communication",children:"Action Server Communication"}),"\n",(0,t.jsx)(n.p,{children:"ROS 2 uses action servers to handle long-running tasks with feedback. Voice commands can trigger these actions through ROS 2 topics and services."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from rclpy.action import ActionClient\nfrom move_base_msgs.action import MoveBase\nfrom geometry_msgs.msg import PoseStamped\n\nclass VoiceActionBridge(Node):\n    def __init__(self):\n        super().__init__('voice_action_bridge')\n        self.subscription = self.create_subscription(\n            String,\n            'voice_commands',\n            self.command_callback,\n            10\n        )\n        self.move_action_client = ActionClient(self, MoveBase, 'move_base')\n\n    def command_callback(self, msg):\n        command = self.parse_command(msg.data)\n        if command and command['action'] == 'move':\n            self.execute_navigation_command(command)\n\n    def execute_navigation_command(self, command):\n        goal_msg = MoveBase.Goal()\n        # Parse location from command and set goal pose\n        # This would involve NLP to extract coordinates or named locations\n        self.move_action_client.send_goal_async(goal_msg)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-considerations",children:"Implementation Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"error-handling-and-feedback",children:"Error Handling and Feedback"}),"\n",(0,t.jsx)(n.p,{children:"Voice-controlled systems must provide clear feedback to users about command recognition and execution status:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visual indicators when listening"}),"\n",(0,t.jsx)(n.li,{children:"Confirmation of understood commands"}),"\n",(0,t.jsx)(n.li,{children:"Status updates during execution"}),"\n",(0,t.jsx)(n.li,{children:"Error notifications when commands fail"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"robustness-and-reliability",children:"Robustness and Reliability"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems must handle:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Background noise and acoustic interference"}),"\n",(0,t.jsx)(n.li,{children:"Different accents and speaking patterns"}),"\n",(0,t.jsx)(n.li,{children:"Partial or incomplete commands"}),"\n",(0,t.jsx)(n.li,{children:"Context-aware command interpretation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems bridge the gap between natural human communication and robotic action execution. By integrating speech recognition with ROS 2's action server framework, we create intuitive interfaces that make robotics accessible to non-technical users while maintaining the precision required for autonomous operation."}),"\n",(0,t.jsx)(n.p,{children:"The next chapter will explore how LLM-based cognitive planning transforms natural language commands into structured action sequences within ROS 2 environments."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);